>   **SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: 30      |
|-----------------|
| Aayush Dahal                |   
| Justin Kuhn              |   
| Sheroze Nasir               |   


**Table of Contents**

[1 Introduction	1](#_Toc439194677)

[2 High-level description of the exploratory testing plan	1](#_Toc439194678)

[3 Comparison of exploratory and manual functional testing	1](#_Toc439194679)

[4 Notes and discussion of the peer reviews of defect reports	1](#_Toc439194680)

[5 How the pair testing was managed and team work/effort was
divided	1](#_Toc439194681)

[6 Difficulties encountered, challenges overcome, and lessons
learned	1](#_Toc439194682)

[7 Comments/feedback on the lab and lab document itself	1](#_Toc439194683)

# Introduction

In this assignment, our group applied fundamentals of testing we learnt in week 1 & week 2 to test an ATM system(v1.0 and v1,1). We applied testing & software engineering knowledge to find faults in the system and reported them in a bug tracking system. The different types of tests performed were Exploratory Testing, Manual Scripted Testing, and Regression Testing. The first step for us was to familiarize ourselves with the system under test (ATM system) as well as the bug tracking system (JIRA).  To familiarize ourselves with the system under test, we read the detailed explanation of the SUT in the lab document and also read the high level requirements of the system under Appendix B. To solidify our understanding, we performed various normal ATM operations on the JAR file of the ATM System. To familiarize ourselves with JIRA, we read an article, “The Ultimate Jira Bug Tracking Tutorial”, and then created an account to familiarize ourselves with the UI and to also try out things we learnt in the article.  Next we performed our first type of test, exploratory tests. Before starting exploratory testing, we decided on a high level plan on how we want to test the system at this stage. We were aware that tests are designed and executed at the same time in exploratory testing without strict plans & constraints, however, we also knew that it was good practice to decide on a high level testing plan before carrying out the testing. Our group had a collective understanding that the point of exploratory testing is for the developer to incrementally create scenarios related to operations of the system to manually test if the software fails or passes the scenario at hand. Next, we performed our second type of test, Manual Scripted Testing. Our group was aware that in scripted testing, a plan with a suite of test cases and steps to execute the test cases are pre written, and the tester is to follow the plan without digressing from it. Lastly, we performed Regression testing. From past courses and experience, our group was familiar with the concept of regression testing. We knew that regression testing is a type of testing mechanism that is used to examine if previously found bugs have been fixed, without creating any new bugs in the existing functionality of the system. During regression testing, our group had an opportunity to test version 1.1 of the system. On this step, we re-tested all bugs from version 1.0 that we reported in JIRA as well as all the 40 test cases from the scripted testing stage. This gave us an opportunity to check if old bugs were fixed in the new version and also if any new bugs were introduced.

# High-level description of the exploratory testing plan

For exploratory testing, we planned on testing most observable functionality of the system with more emphasis on core functionalities that define the system and are a must for the system to function. For instance, some questions we wanted to answer were, is the correct amount transferred from one account to another?, does it transfer funds to & from the correct account? etc. We also focused on testing the common paths of the system, i.e, combinations and order of inputs that users are most likely to use day-to-day in an ATM system. The most common paths are operations that users will most frequently encounter and are what defines a system. Testing most common paths/core functionality first then focusing on the nice-to-haves goes well with the idea of the popular agile manifesto as this approach allows for delivery of a functional product first. Since, most bugs live on the edge, we also put emphasis on testing edge cases. Working as a team works well with the exploratory testing method. This is because each team member can focus on a different portion of the system under test, and afterwards compare their results to produce bug reports of the whole system in a quick manner. The purpose of exploratory testing is not to debug every possible input in every possible state, but rather to verify that the core functionalities that define the system work as intended.

# Comparison of exploratory and manual functional testing

In general, exploratory testing presents a more broad perspective of software testing. It is not exhaustive, and its primary goal is to test the core functionalities that a system is built off of. It does not follow a script, but rather operates by users testing whatever comes to mind, as long as it is logical to do so. Conversely, manual functional testing is the strict following of a script in order to perform tests. This is where individual functions get tested repeatedly with varying inputs, and system states are explored in much more depth. Relative to the provided test suite, the given test cases were more specific than the exploratory tests, and involved putting oneself in situations that users would rarely encounter during a normal run of the program. For example, one of the test cases was “User enters correct pin on third try”, and another was “A deposit transaction can be canceled by the customer at any time prior to inserting an envelope”. These situations are more obscure, and did not occur to any group member during the exploratory test phase. Comparing these two testing methods, each one has advantages/disadvantages over the other. Exploratory testing is faster and overall less demanding, but manual functional testing is more thorough, complete and will likely find more bugs in the end. Manual functional testing tries to be exhaustive. Even though it can’t test every possible input, it spends a good amount of time on edge cases and values that users wouldn’t normally enter. Deciding which method of testing is better overall is subjective and varies on a case-to-case basis. In an agile team, where working software is demanded at a fast pace, exploratory testing may be sufficient as there is more emphasis on the core functionalities of the system. Additionally, in an environment where developers are receiving constant feedback, there will always be opportunity to fix bugs in the loop of sprint cycles. However, in a waterfall team, where customer feedback is not received until the end of the project, manual functional testing is required as it is essential that the final product is well-tested and as bug-free as it can be. As well, large scale projects that involve sensitive client information, such as a banking system should also be tested with manual functional testing. This is because bugs/exploits that are not patched may have a catastrophic effect, and customers will not feel secure using software that is not thoroughly tested. In our testing, we found that exploratory testing found more bugs, however, performing regression tests was easier on manual scripted tests since they were well-documented. 

# Notes and discussion of the peer reviews of defect reports


# How the pair testing was managed and team work/effort was divided 

Since, we had 3 members in our group, we took an adjusted approach for pair testing as well as the whole group assignment. For pair testing, we decided to make one group of 2 testers and then another individual tester to test on their own. Before performing the Exploratory Tests, we collectively came up with a high level testing plan as well as some high level test cases we wanted to ensure were covered. Then, we went off and started testing! When both groups completed performing the exploratory tests and reporting bugs in JIRA, we came together as a group and discussed and ran through all the test cases each group tested and the bug report for each bug found. Through the exploratory testing phase, each team wrote down and kept track of all the test cases they performed and all of those that failed in a Google Doc to keep organized and to help aid our discussions.

For Manual Script Testing, we split into our 2 people and 1 people group as before, and assigned one group scripted test cases 1-20 and the other group scripted test cases 21-40. If a bug was encountered in the system by any group for any test case, a detailed bug report was written in JIRA with “MFT” on the title with the description of the bug, the function being tested, how to reproduce, expected and actual outcome etc. 

During the Regression Testing stage, we again split up into our 2 people and 1 people group. Each group was assigned to perform regression testing on version 1.1 of the system with all test cases they tested in stage 1(exploratory testing) and stage 2(manual scripted testing). If a previously failed test case was fixed in version 1.1, the bug report in JIRA of the corresponding bug got marked “DONE”. If a previously failed test case was not fixed in version 1.1, the bug report in JIRA of the corresponding bug got marked “IN PROGRESS” and a note stating that the bug still exists in version 1.1 was added by members performing regression testing,

Note: One principle we tried to follow when dividing work was, to the extent the circumstances allow, give team members the freedom to pick work that they expertise in and they enjoy doing.

# Difficulties encountered, challenges overcome, and lessons learned

Text…

# Comments/feedback on the lab and lab document itself

Text…
